from assembler.gaf_reader import GafReader
#from assembler.builder import AnchorDictionary
from assembler.aligner import AlignAnchor
import assembler.parser as lp
import time
from sys import stderr
import os


class Orchestrator:

    def __init__(
        self, dictionary_path: str, graph_path: str, gaf_path: str, fasta_path: str
    ):
        """
        It initiailzes the AlignAnchor object with the packedgraph path and the dictionary generated by the assembler.builder.AnchorDictionrary object.
        It initializes the GafReader object that reads the gaf file.

        Parameters
        ----------
        sentinel_to_anchor_dictionary: dictionary
            the dctionary associating sentinels and anchors
        graph_path: string
            The filepath of the packedGraph object
        gaf_path:
            The filepath of the gaf alignment file
        fasta_path: string
            The filepath of the reads fasta file
        """
        self.alignment_processor = AlignAnchor()
        self.alignment_processor.build(dictionary_path, graph_path)
        self.gaf_reader = GafReader(gaf_path)
        self.alignment_processor.readFasta(fasta_path)

    def process(self, debug_outfile):
        """
        It reads the gaf file line by line and if the line is valid and not already processed (there could be duplicates), it processes it to find anchors that align to it.
        """
        times = []
        total_reads_in_gaf = 0
        remove_duplicates = set()
        reads_out_file = debug_outfile + ".reads_processed.tsv"
        # remove reads_out_file file if it exists
        if os.path.exists(reads_out_file):
            os.remove(reads_out_file)
        with open(f"{debug_outfile}.read_anchor.csv", "w") as debug:
            print("READ_ID,ANCHOR,IS_MATCHING_NODES,IS_BASELEVEL_ALIGNED", file=debug)
            for count, line in enumerate(self.gaf_reader.get_lines()):
                # print(f"Processing line {count}",flush=True,file=stderr)
                if line not in remove_duplicates:
                    remove_duplicates.add(line)
                    t0 = time.time()
                    parsed_data = lp.processGafLine(line, reads_out_file)
                    if parsed_data:
                        # print(f"PROCESSING READ {parsed_data[0]} ...")
                        self.alignment_processor.processGafLine(parsed_data, debug)
                        t1 = time.time()
                        # print(f"Done in {t1-t0}.", file=stderr)
                        times.append(t1-t0)
                total_reads_in_gaf+=1

        # self.alignment_processor.dump_anchor_information(f"{debug_outfile}.anchors_zygosity.tsv")

        print(f"Out of {total_reads_in_gaf} alignments in the GAF file, {len(times)} alignments are unique")
        print(f"Processed {len(times)} alignments in {sum(times):.4f}. {sum(times)/len(times):.4f} per alignment")
        print(f"Anchors-Reads path matches = {self.alignment_processor.reads_matching_anchor_path}, sequence matches = {self.alignment_processor.reads_matching_anchor_sequence}.")
        if (self.alignment_processor.reads_matching_anchor_path != 0): 
            print(f"Ratio = {(self.alignment_processor.reads_matching_anchor_sequence/self.alignment_processor.reads_matching_anchor_path):.2f}")

    def dump_anchors(self, out_file: str, extended_out_file: str, anchor_read_tracking_file_path: str, independent_anchor_read_tracking_file_path: str, extended_pruned_out_file: str, reliable_snarls_out_file_path: str, snarl_variant_type_out_file_path: str, snarl_compatibility_out_file_path: str, snarl_common_reads_out_file_path: str, snarl_read_partitions_out_file_path: str, snarl_coverage_out_file_path: str, snarl_allelic_coverage_out_file_path: str, snarl_coverage_extended_out_file_path: str, snarl_allelic_coverage_extended_out_file_path: str):
        """
        It dumps the anchors by json
        """
        self.alignment_processor.dump_valid_anchors(out_file, extended_out_file, anchor_read_tracking_file_path, independent_anchor_read_tracking_file_path, extended_pruned_out_file, reliable_snarls_out_file_path, snarl_variant_type_out_file_path, snarl_compatibility_out_file_path, snarl_common_reads_out_file_path, snarl_read_partitions_out_file_path, snarl_coverage_out_file_path, snarl_allelic_coverage_out_file_path, snarl_coverage_extended_out_file_path, snarl_allelic_coverage_extended_out_file_path)

    def dump_dictionary_with_counts(self, out_file: str):
        """
        It dumps the positioned anchor dictionary by json
        """
        self.alignment_processor.dump_dictionary_with_reads_counts(out_file)

    def dump_dict_size_extended(self, out_file: str):
        """
        It dumps the anchors by json
        """
        self.alignment_processor.print_extended_anchor_info(out_file) 

    def dump_bandage_csv_extended(self, out_file: str):
        """
        It dumps CSV with node and colour of all anchor nodes
        """
        self.alignment_processor.print_sentinels_for_bandage(out_file) 

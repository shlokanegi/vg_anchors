from assembler.gaf_reader import GafReader
#from assembler.builder import AnchorDictionary
from assembler.aligner import AlignAnchor
import assembler.parser as lp
import time
from sys import stderr


class Orchestrator:

    def __init__(
        self, dictionary_path: str, graph_path: str, gaf_path: str
    ):
        """
        It initiailzes the AlignAnchor object with the packedgraph path and the dictionary generated by the assembler.builder.AnchorDictionrary object.
        It initializes the GafReader object that reads the gaf file.

        Parameters
        ----------
        sentinel_to_anchor_dictionary: dictionary
            the dctionary associating sentinels and anchors
        graph_path: string
            The filepath of the packedGraph object
        gaf_path:
            The filepath of the gaf alignment file
        """
        self.alignment_processor = AlignAnchor()
        self.alignment_processor.build(dictionary_path, graph_path)
        self.gaf_reader = GafReader(gaf_path)

    def process(self, debug_outfile):
        """
        It reads the gaf file line by line and if the line is valid and not already processed (there could be duplicates), it processes it to find anchors that align to it.
        """
        times = []
        total_reads_in_gaf = 0
        remove_duplicates = set()
        reads_out_file = debug_outfile + ".reads_processed.tsv"
        with open(f"{debug_outfile}.read_anchor.csv", "w") as debug:
            print("READ_ID,ANCHOR,IS_MATCHING_NODES,IS_BASELEVEL_ALIGNED", file=debug)
            for count, line in enumerate(self.gaf_reader.get_lines()):
                # print(f"Processing line {count}",flush=True,file=stderr)
                if line not in remove_duplicates:
                    remove_duplicates.add(line)
                    t0 = time.time()
                    parsed_data = lp.processGafLine(line, reads_out_file)
                    if parsed_data:
                        print(f"PROCESSING READ {parsed_data[0]} ...")
                        self.alignment_processor.processGafLine(parsed_data, debug)
                        t1 = time.time()
                        print(f"Done in {t1-t0}.", file=stderr)
                        times.append(t1-t0)
                total_reads_in_gaf+=1

        # self.alignment_processor.dump_anchor_information(f"{debug_outfile}.anchors_zygosity.tsv")

        print(f"Out of {total_reads_in_gaf} alignments in the GAF file, {len(times)} alignments are unique")
        print(f"Processed {len(times)} alignments in {sum(times):.4f}. {sum(times)/len(times):.4f} per alignment")
        print(f"Anchors-Reads path matches = {self.alignment_processor.reads_matching_anchor_path}, sequence matches = {self.alignment_processor.reads_matching_anchor_sequence}.")
        if (self.alignment_processor.reads_matching_anchor_path != 0): 
            print(f"Ratio = {(self.alignment_processor.reads_matching_anchor_sequence/self.alignment_processor.reads_matching_anchor_path):.2f}")

    def dump_anchors(self, out_file: str, extended_out_file: str):
        """
        It dumps the anchors by json
        """
        self.alignment_processor.dump_valid_anchors(out_file, extended_out_file)

    def dump_dictionary_with_counts(self, out_file: str):
        """
        It dumps the positioned anchor dictionary by json
        """
        self.alignment_processor.dump_dictionary_with_reads_counts(out_file)

    def dump_dict_size_extended(self, out_file: str):
        """
        It dumps the anchors by json
        """
        self.alignment_processor.print_extended_anchor_info(out_file) 

    def dump_bandage_csv_extended(self, out_file: str):
        """
        It dumps CSV with node and colour of all anchor nodes
        """
        self.alignment_processor.print_sentinels_for_bandage(out_file) 
